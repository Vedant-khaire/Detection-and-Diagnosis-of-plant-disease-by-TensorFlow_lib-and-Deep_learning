{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc63bc1c-0be1-490c-947e-b7ad55f97240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Wilting images for training: 100%|█████████████████████████████████████████████| 10/10 [00:00<00:00, 66.64it/s]\n",
      "Loading Rust images for training: 100%|████████████████████████████████████████████████| 20/20 [00:02<00:00,  9.34it/s]\n",
      "Loading Rose_Gall images for training: 100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 91.50it/s]\n",
      "Loading Powdery images for training: 100%|█████████████████████████████████████████████| 20/20 [00:01<00:00, 10.20it/s]\n",
      "Loading Peach_Leaf_Curl images for training: 100%|█████████████████████████████████████| 11/11 [00:00<00:00, 91.72it/s]\n",
      "Loading Healthy images for training: 100%|█████████████████████████████████████████████| 20/20 [00:01<00:00, 10.14it/s]\n",
      "Loading Fungal_Diseases images for training: 100%|█████████████████████████████████████| 11/11 [00:00<00:00, 89.37it/s]\n",
      "Loading Environmental_Stress images for training: 100%|████████████████████████████████| 11/11 [00:00<00:00, 97.49it/s]\n",
      "Loading Crown_Gall_Disease images for training: 100%|█████████████████████████████████| 12/12 [00:00<00:00, 109.79it/s]\n",
      "Loading Bacterial_Infections images for training: 100%|████████████████████████████████| 11/11 [00:00<00:00, 44.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the path to your dataset\n",
    "dataset_path = \"database\"\n",
    "\n",
    "# List of classes (disease names)\n",
    "classes = [\"Wilting\", \"Rust\", \"Rose_Gall\", \"Powdery\", \"Peach_Leaf_Curl\", \"Healthy\", \"Fungal_Diseases\", \"Environmental_Stress\", \"Crown_Gall_Disease\", \"Bacterial_Infections\"]\n",
    "\n",
    "# Dictionary to store images and their labels\n",
    "data = {\"images\": [], \"labels\": []}\n",
    "\n",
    "# Define a common size for all images (adjust as needed)\n",
    "target_size = (100, 100)\n",
    "\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(dataset_path, \"Train\", class_name)\n",
    "    class_label = classes.index(class_name)\n",
    "\n",
    "    for img_file in tqdm(os.listdir(class_path), desc=f\"Loading {class_name} images for training\"):\n",
    "        img_path = os.path.join(class_path, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        # Resize the image to a common size\n",
    "        img = cv2.resize(img, target_size)\n",
    "\n",
    "        # Perform any additional preprocessing if needed (normalization, etc.)\n",
    "\n",
    "        data[\"images\"].append(img)\n",
    "        data[\"labels\"].append(class_label)\n",
    "\n",
    "# Convert lists to NumPy arrays for further processing\n",
    "train_images = np.array(data[\"images\"])\n",
    "train_labels = np.array(data[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "356642fc-0635-4db7-aca3-c492465ca1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 168ms/step - loss: 2.5006 - accuracy: 0.1284 - val_loss: 2.2800 - val_accuracy: 0.1071\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 131ms/step - loss: 2.2605 - accuracy: 0.1651 - val_loss: 2.2131 - val_accuracy: 0.2143\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 122ms/step - loss: 2.1796 - accuracy: 0.1376 - val_loss: 2.1000 - val_accuracy: 0.2500\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 123ms/step - loss: 2.0119 - accuracy: 0.2385 - val_loss: 2.0934 - val_accuracy: 0.2143\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 125ms/step - loss: 1.8156 - accuracy: 0.3211 - val_loss: 2.0351 - val_accuracy: 0.1429\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 124ms/step - loss: 1.6091 - accuracy: 0.3578 - val_loss: 2.0879 - val_accuracy: 0.1786\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 123ms/step - loss: 1.4590 - accuracy: 0.5413 - val_loss: 2.2405 - val_accuracy: 0.1429\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 118ms/step - loss: 1.3713 - accuracy: 0.5046 - val_loss: 1.9400 - val_accuracy: 0.2143\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 117ms/step - loss: 1.0860 - accuracy: 0.6697 - val_loss: 1.8171 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 126ms/step - loss: 0.8990 - accuracy: 0.7706 - val_loss: 1.8113 - val_accuracy: 0.2857\n",
      "[0.12844036519527435, 0.1651376187801361, 0.1376146823167801, 0.23853211104869843, 0.3211009204387665, 0.35779815912246704, 0.5412843823432922, 0.5045871734619141, 0.6697247624397278, 0.7706422209739685]\n",
      "[0.1071428582072258, 0.2142857164144516, 0.25, 0.2142857164144516, 0.1428571492433548, 0.1785714328289032, 0.1428571492433548, 0.2142857164144516, 0.5, 0.2857142984867096]\n",
      "[2.5006487369537354, 2.2605154514312744, 2.1796154975891113, 2.0119311809539795, 1.8155564069747925, 1.6090985536575317, 1.45896577835083, 1.371260643005371, 1.0859512090682983, 0.8990278840065002]\n",
      "[2.280033826828003, 2.2130746841430664, 2.099963426589966, 2.0934433937072754, 2.0351226329803467, 2.0879361629486084, 2.240461826324463, 1.9399588108062744, 1.8171217441558838, 1.8113346099853516]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train, X_val = X_train / 255.0, X_val / 255.0\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))  # Assuming you have 10 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Access training history\n",
    "print(history.history['accuracy'])\n",
    "print(history.history['val_accuracy'])\n",
    "print(history.history['loss'])\n",
    "print(history.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796cfa5-81e1-41de-8d67-636e9fadb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def load_and_predict(model, image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))  # Adjust target_size based on your model's input size\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.  # Normalize pixel values to between 0 and 1\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    return prediction\n",
    "\n",
    "# Replace 'your_model.h5' with the actual path to your trained model file\n",
    "model = load_model('your_model.h5')  \n",
    "\n",
    "# Replace 'path/to/test/images/' with the actual path to your test images\n",
    "test_image_folder = 'Database/Test'  \n",
    "\n",
    "# Replace 'class_labels.txt' with the actual path to your class labels file\n",
    "class_labels_path = 'class_labels.txt' \n",
    "\n",
    "with open(class_labels_path, 'r') as file:\n",
    "    class_labels = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Replace 'image1.jpg', 'image2.jpg', ... with actual image filenames\n",
    "image_filenames = ['image1.jpg', 'image2.jpg', 'image3.jpg']\n",
    "\n",
    "for filename in image_filenames:\n",
    "    image_path = os.path.join(test_image_folder, filename)\n",
    "    prediction = load_and_predict(model, image_path)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    class_name = class_labels[predicted_class]\n",
    "\n",
    "    # Visualization code (you can customize this based on your preferences)\n",
    "    img = image.load_img(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Predicted Class: {class_name}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445762be-6155-4f70-b3ac-b1cdf8c4f50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
